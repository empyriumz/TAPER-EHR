{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test gen_data_df.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def read_patients_table():\n",
    "    p = pd.read_csv(\"data/PATIENTS.csv.gz\")\n",
    "    p = p[\n",
    "        [\n",
    "            \"SUBJECT_ID\",\n",
    "            \"GENDER\",\n",
    "            \"DOB\",\n",
    "            \"DOD\",\n",
    "        ]\n",
    "    ]\n",
    "    p[\"DOB\"] = pd.to_datetime(p[\"DOB\"])\n",
    "    p[\"DOD\"] = pd.to_datetime(p[\"DOD\"])\n",
    "    return p\n",
    "\n",
    "def read_icd_diagnoses_table():\n",
    "    codes = pd.read_csv(\"data/D_ICD_DIAGNOSES.csv.gz\")\n",
    "    codes = codes[[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"]]\n",
    "    diagnoses = pd.read_csv(\"data/DIAGNOSES_ICD.csv.gz\")\n",
    "    diagnoses = diagnoses.merge(\n",
    "        codes, how=\"inner\", left_on=\"ICD9_CODE\", right_on=\"ICD9_CODE\"\n",
    "    )\n",
    "    diagnoses[[\"SUBJECT_ID\", \"HADM_ID\", \"SEQ_NUM\"]] = diagnoses[\n",
    "        [\"SUBJECT_ID\", \"HADM_ID\", \"SEQ_NUM\"]\n",
    "    ].astype(int)\n",
    "    return diagnoses\n",
    "\n",
    "def read_icd_procedures_table():\n",
    "    codes = pd.read_csv(\"data/D_ICD_PROCEDURES.csv.gz\")\n",
    "    codes = codes[[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"]]\n",
    "    procedures = pd.read_csv(\"data/PROCEDURES_ICD.csv.gz\")\n",
    "    procedures = procedures.merge(\n",
    "        codes, how=\"inner\", left_on=\"ICD9_CODE\", right_on=\"ICD9_CODE\"\n",
    "    )\n",
    "    procedures[[\"SUBJECT_ID\", \"HADM_ID\", \"SEQ_NUM\"]] = procedures[\n",
    "        [\"SUBJECT_ID\", \"HADM_ID\", \"SEQ_NUM\"]\n",
    "    ].astype(int)\n",
    "    return procedures\n",
    "\n",
    "def read_cptevents_table():\n",
    "    cpt = pd.read_csv(\"data/CPTEVENTS.csv.gz\")\n",
    "    cpt = cpt[\n",
    "        [\n",
    "            \"SUBJECT_ID\",\n",
    "            \"HADM_ID\",\n",
    "            \"CPT_CD\",\n",
    "        ]\n",
    "    ]\n",
    "    return cpt\n",
    "\n",
    "def read_prescriptions_table():\n",
    "    prescription = pd.read_csv(\"data/PRESCRIPTIONS.csv.gz\")\n",
    "    prescription = prescription[~prescription[\"NDC\"].isna()]\n",
    "    prescription = prescription[[\"SUBJECT_ID\", \"HADM_ID\", \"NDC\"]].astype(int)\n",
    "    prescription = prescription.dropna()\n",
    "    return prescription\n",
    "\n",
    "def read_icustays_table():\n",
    "    icu = pd.read_csv(\"data/ICUSTAYS.csv.gz\")\n",
    "    icu[\"INTIME\"] = pd.to_datetime(icu[\"INTIME\"])\n",
    "    icu[\"OUTTIME\"] = pd.to_datetime(icu[\"OUTTIME\"])\n",
    "    return icu\n",
    "\n",
    "DataFrame = pd.DataFrame\n",
    "\n",
    "def filter_codes(df, code:str, min_=5, max_=np.inf) -> DataFrame:\n",
    "    t = df.groupby(code)[code].transform(len) > min_\n",
    "    num_codes = len(set(df[code]))\n",
    "    num_codes_after = len(set(df.loc[t, code]))\n",
    "    print(\n",
    "        \"removing {} codes occuring less than {} times. \\n num codes before filter: {} after filtering: {}\".format(\n",
    "            code, min_, num_codes, num_codes_after\n",
    "        )\n",
    "    )\n",
    "    return df[t]\n",
    "\n",
    "def remove_min_admissions(t, min_admits=1):\n",
    "    tt = t.groupby(\"SUBJECT_ID\").SUBJECT_ID.transform(len) >= min_admits\n",
    "    t = t[tt]\n",
    "    print(\n",
    "        \"num of subjects with min_admits of {} is {}\".format(\n",
    "            min_admits, len(set(t[\"SUBJECT_ID\"]))\n",
    "        )\n",
    "    )\n",
    "    return t\n",
    "\n",
    "def group_by_return_col_list(t, groupby, col, col_name=\"\"):\n",
    "    if col_name == \"\":\n",
    "        col_name = col\n",
    "    return (\n",
    "        t.groupby(groupby)\n",
    "        .apply(lambda x: x[col].values.tolist())\n",
    "        .reset_index(name=col_name)\n",
    "    )\n",
    "\n",
    "def merge_on_subject(\n",
    "    t1,\n",
    "    t2,\n",
    "    how=\"left\",\n",
    "    left_on=[\"SUBJECT_ID\", \"HADM_ID\"],\n",
    "    right_on=[\"SUBJECT_ID\", \"HADM_ID\"],\n",
    "):\n",
    "    return t1.merge(\n",
    "        t2,\n",
    "        how=how,\n",
    "        left_on=left_on,\n",
    "        right_on=right_on,\n",
    "    )\n",
    "\n",
    "def add_age_to_icustays(stays):\n",
    "    dob = pd.to_datetime(stays[\"DOB\"])\n",
    "    dob = dob.values.astype(\"datetime64[s]\")\n",
    "    intime = pd.to_datetime(stays[\"INTIME\"])\n",
    "    intime = intime.values.astype(\"datetime64[s]\")\n",
    "    age = intime - dob\n",
    "    g = lambda x: x / np.timedelta64(1, \"s\") / 60 / 60 / 24 / 365\n",
    "    stays[\"AGE\"] = np.asarray(list(map(g, age)))\n",
    "    idxs = stays.AGE < 0\n",
    "    stays.loc[idxs, \"AGE\"] = 90\n",
    "    return stays\n",
    "\n",
    "### If Less than n days on admission notes (Early notes)\n",
    "def less_n_days_data(df_adm_notes, n):\n",
    "    df_less_n = df_adm_notes[\n",
    "        (\n",
    "            (\n",
    "                df_adm_notes[\"CHARTDATE\"] - df_adm_notes[\"ADMITTIME_C\"]\n",
    "            ).dt.total_seconds()\n",
    "            / (24 * 60 * 60)\n",
    "        )\n",
    "        < n\n",
    "    ]\n",
    "    df_less_n = df_less_n[df_less_n[\"TEXT\"].notnull()]\n",
    "    return df_less_n\n",
    "\n",
    "def preprocess1(x):\n",
    "    y = re.sub(\"\\\\[(.*?)\\\\]\", \"\", x)  # remove de-identified brackets\n",
    "    y = re.sub(\n",
    "        \"[0-9]+\\.\", \"\", y\n",
    "    )  # remove 1.2. since the segmenter segments based on this\n",
    "    y = re.sub(\"dr\\.\", \"doctor\", y)\n",
    "    y = re.sub(\"m\\.d\\.\", \"md\", y)\n",
    "    y = re.sub(\"admission date:\", \"\", y)\n",
    "    y = re.sub(\"discharge date:\", \"\", y)\n",
    "    y = re.sub(\"--|__|==\", \"\", y)\n",
    "    return y\n",
    "\n",
    "def preprocessing(df, col = 'TEXT'):\n",
    "    df[col] = df[col].fillna(\" \")\n",
    "    df[col] = df[col].str.replace(\"\\n\", \" \")\n",
    "    df[col] = df[col].str.replace(\"\\r\", \" \")\n",
    "    df[col] = df[col].apply(str.strip)\n",
    "    df[col] = df[col].str.lower()\n",
    "    df[col] = df[col].apply(lambda x: preprocess1(x))\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x.split()))\n",
    "    return df\n",
    "\n",
    "def append_text(df):\n",
    "    group = df.groupby('HADM_ID')\n",
    "    df_list = []\n",
    "    for idx in group.groups.values():\n",
    "        tmp = df.loc[idx]\n",
    "        discharge = tmp[tmp[\"CATEGORY\"] == \"Discharge summary\"]\n",
    "        rest = tmp[tmp[\"CATEGORY\"] != \"Discharge summary\"]\n",
    "        discharge = \" \".join(discharge[\"TEXT\"])\n",
    "        rest = \" \".join(rest[\"TEXT\"])\n",
    "        tmp = tmp.iloc[0]\n",
    "        tmp['TEXT_DISCHARGE'] = discharge\n",
    "        tmp['TEXT_REST'] = rest\n",
    "        df_list.append(tmp)\n",
    "    \n",
    "    df = pd.concat(df_list, axis = 1)\n",
    "    return df.T\n",
    "\n",
    "def compute_time_delta(df):\n",
    "    df[\"TIMEDELTA\"] = (\n",
    "        df.sort_values([\"SUBJECT_ID\", \"ADMITTIME\"])\n",
    "        .groupby([\"SUBJECT_ID\"])[\"ADMITTIME\"]\n",
    "        .diff()\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all data including text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm = pd.read_csv(\"data/ADMISSIONS.csv.gz\")\n",
    "df_adm.ADMITTIME = pd.to_datetime(\n",
    "  df_adm.ADMITTIME, format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\"\n",
    " )\n",
    "df_adm.DISCHTIME = pd.to_datetime(\n",
    "  df_adm.DISCHTIME, format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\"\n",
    " )\n",
    "df_adm.DEATHTIME = pd.to_datetime(\n",
    "  df_adm.DEATHTIME, format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm = df_adm.sort_values([\"SUBJECT_ID\", \"ADMITTIME\"])\n",
    "df_adm = df_adm.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df_adm['SUBJECT_ID'].values\n",
    "print(len(ids), len(set(ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm[\"NEXT_ADMITTIME\"] = df_adm.groupby(\"SUBJECT_ID\").ADMITTIME.shift(-1)\n",
    "df_adm[\"NEXT_ADMISSION_TYPE\"] = df_adm.groupby(\"SUBJECT_ID\").ADMISSION_TYPE.shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = df_adm.NEXT_ADMISSION_TYPE == \"ELECTIVE\"\n",
    "df_adm.loc[rows, \"NEXT_ADMITTIME\"] = pd.NaT\n",
    "df_adm.loc[rows, \"NEXT_ADMISSION_TYPE\"] = np.NaN\n",
    "df_adm = df_adm.sort_values([\"SUBJECT_ID\", \"ADMITTIME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm[\"DAYS_NEXT_ADMIT\"] = (\n",
    "        df_adm.NEXT_ADMITTIME - df_adm.DISCHTIME\n",
    "    ).dt.total_seconds() / (24 * 60 * 60)\n",
    "df_adm[\"readmission_label\"] = (df_adm.DAYS_NEXT_ADMIT < 30).astype(\"int\")\n",
    "### filter out newborn and death\n",
    "df_adm = df_adm[df_adm[\"ADMISSION_TYPE\"] != \"NEWBORN\"]\n",
    "df_adm[\"DURATION\"] = (\n",
    "        df_adm[\"DISCHTIME\"] - df_adm[\"ADMITTIME\"]\n",
    "    ).dt.total_seconds() / (24 * 60 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes = pd.read_csv(\"data/NOTEEVENTS.csv.gz\")\n",
    "df_notes = df_notes.sort_values(by=[\"SUBJECT_ID\", \"HADM_ID\", \"CHARTDATE\"])\n",
    "\n",
    "df_adm_notes = pd.merge(\n",
    "        df_adm[\n",
    "            [\n",
    "                \"SUBJECT_ID\",\n",
    "                \"HADM_ID\",\n",
    "                \"ADMITTIME\",\n",
    "                \"DISCHTIME\",\n",
    "                \"DAYS_NEXT_ADMIT\",\n",
    "                \"NEXT_ADMITTIME\",\n",
    "                \"ADMISSION_TYPE\",\n",
    "                \"DEATHTIME\",\n",
    "                \"readmission_label\",\n",
    "                \"DURATION\",\n",
    "                \"DIAGNOSIS\",\n",
    "                \"MARITAL_STATUS\",\n",
    "                \"ETHNICITY\",\n",
    "                \"DISCHARGE_LOCATION\",\n",
    "            ]\n",
    "        ],\n",
    "        df_notes[[\"SUBJECT_ID\", \"HADM_ID\", \"CHARTDATE\", \"TEXT\", \"CATEGORY\"]],\n",
    "        on=[\"SUBJECT_ID\", \"HADM_ID\"],\n",
    "        how=\"left\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding clinical codes to dataset\n",
    "\n",
    "# add diagnoses\n",
    "code = \"ICD9_CODE\"\n",
    "diagnoses = read_icd_diagnoses_table()\n",
    "diagnoses = filter_codes(diagnoses, code = code)\n",
    "diagnoses = group_by_return_col_list(\n",
    "            diagnoses, [\"SUBJECT_ID\", \"HADM_ID\"], code\n",
    "            )\n",
    "\n",
    "# add procedures\n",
    "procedures = read_icd_procedures_table()\n",
    "procedures = filter_codes(procedures, code = code)\n",
    "procedures = group_by_return_col_list(\n",
    "            procedures, [\"SUBJECT_ID\", \"HADM_ID\"], code, \"ICD9_CODE_PROCEDURE\"\n",
    "            )\n",
    "\n",
    "# add cptevents\n",
    "code = \"CPT_CD\"\n",
    "cptevents = read_cptevents_table()\n",
    "cptevents = filter_codes(cptevents, code = code)\n",
    "cptevents = group_by_return_col_list(cptevents, [\"SUBJECT_ID\", \"HADM_ID\"], code)\n",
    "\n",
    "# add prescriptions\n",
    "code = \"NDC\"\n",
    "prescriptions = read_prescriptions_table()\n",
    "prescriptions = filter_codes(prescriptions, code = code)\n",
    "prescriptions = group_by_return_col_list(\n",
    "                prescriptions, [\"SUBJECT_ID\", \"HADM_ID\"], code\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = read_patients_table()\n",
    "stays = read_icustays_table()\n",
    "stays = merge_on_subject(\n",
    "        stays, patients, how=\"inner\", left_on=[\"SUBJECT_ID\"], right_on=[\"SUBJECT_ID\"]\n",
    ")\n",
    "stays = merge_on_subject(stays, diagnoses)\n",
    "stays = merge_on_subject(stays, cptevents)\n",
    "stays = merge_on_subject(stays, prescriptions)\n",
    "stays = merge_on_subject(stays, procedures)\n",
    "stays = add_age_to_icustays(stays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters=[\n",
    "            \"Discharge summary\",\n",
    "            \"ECG\",\n",
    "            \"Pharmacy\",\n",
    "            \"Physician\",\n",
    "            \"Radiology\",\n",
    "            \"Respiratory\",\n",
    "        ]\n",
    "\n",
    "df_adm_notes = pd.merge(\n",
    "                df_adm_notes, stays, on=[\"SUBJECT_ID\", \"HADM_ID\"], how=\"left\"\n",
    "            )\n",
    "filt = df_adm_notes[\"ICD9_CODE\"].isna() & df_adm_notes[\"CPT_CD\"].isna()\n",
    "df_adm_notes = df_adm_notes[~filt]\n",
    "\n",
    "df_adm_notes[\"ADMITTIME_C\"] = df_adm_notes.ADMITTIME.apply(lambda x: str(x).split(\" \")[0])\n",
    "df_adm_notes[\"ADMITTIME_C\"] = pd.to_datetime(\n",
    "                                df_adm_notes.ADMITTIME_C, \n",
    "                                format=\"%Y-%m-%d\", \n",
    "                                errors=\"coerce\"\n",
    "                            )\n",
    "df_adm_notes[\"CHARTDATE\"] = pd.to_datetime(\n",
    "                            df_adm_notes.CHARTDATE, \n",
    "                            format=\"%Y-%m-%d\", \n",
    "                            errors=\"coerce\"\n",
    "                            )\n",
    "\n",
    "filt = df_adm_notes[\"CATEGORY\"].apply(lambda x: x in filters)\n",
    "df_adm_notes = df_adm_notes[filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If Discharge Summary\n",
    "df_discharge = df_adm_notes[df_adm_notes[\"CATEGORY\"] == \"Discharge summary\"]\n",
    "\n",
    "# multiple discharge summary for one admission -> after examination -> replicated summary -> replace with the last one\n",
    "df_discharge = (\n",
    "    df_discharge.groupby([\"SUBJECT_ID\", \"HADM_ID\"]).nth(-1)\n",
    ").reset_index()\n",
    "df_discharge = df_discharge[df_discharge[\"TEXT\"].notnull()]\n",
    "df_discharge = remove_min_admissions(df_discharge, min_admits=2)\n",
    "df_adm_notes = df_adm_notes[df_adm_notes[\"CATEGORY\"] != \"Discharge summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_less_1 = less_n_days_data(df_adm_notes, 1)\n",
    "df_less_2 = less_n_days_data(df_adm_notes, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_less_1 = df_less_1.append(df_discharge).reset_index()\n",
    "df_less_1 = preprocessing(df_less_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm_notes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm_notes.TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data without text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format date time\n",
    "df_adm = pd.read_csv(\"data/ADMISSIONS.csv.gz\")\n",
    "df_adm.ADMITTIME = pd.to_datetime(\n",
    "    df_adm.ADMITTIME, format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\"\n",
    ")\n",
    "df_adm.DISCHTIME = pd.to_datetime(\n",
    "    df_adm.DISCHTIME, format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\"\n",
    ")\n",
    "df_adm.DEATHTIME = pd.to_datetime(\n",
    "    df_adm.DEATHTIME, format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\"\n",
    ")\n",
    "\n",
    "df_adm = df_adm.sort_values([\"SUBJECT_ID\", \"ADMITTIME\"])\n",
    "df_adm = df_adm.reset_index(drop=True)\n",
    "# one task in the paper is to predict re-admission within 30 days\n",
    "df_adm[\"NEXT_ADMITTIME\"] = df_adm.groupby(\"SUBJECT_ID\").ADMITTIME.shift(periods=-1)\n",
    "df_adm[\"NEXT_ADMISSION_TYPE\"] = df_adm.groupby(\"SUBJECT_ID\").ADMISSION_TYPE.shift(\n",
    "    periods=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = df_adm.NEXT_ADMISSION_TYPE == \"ELECTIVE\"\n",
    "df_adm.loc[rows, \"NEXT_ADMITTIME\"] = pd.NaT\n",
    "df_adm.loc[rows, \"NEXT_ADMISSION_TYPE\"] = np.NaN\n",
    "\n",
    "df_adm = df_adm.sort_values([\"SUBJECT_ID\",\"ADMITTIME\"])\n",
    "\n",
    "# When we filter out the \"ELECTIVE\",\n",
    "# we need to correct the next admit time\n",
    "# for these admissions since there might\n",
    "# be 'emergency' next admit after \"ELECTIVE\"\n",
    "df_adm[[\"NEXT_ADMITTIME\", \"NEXT_ADMISSION_TYPE\"]] = df_adm.groupby([\"SUBJECT_ID\"])[\n",
    "    [\"NEXT_ADMITTIME\", \"NEXT_ADMISSION_TYPE\"]\n",
    "].fillna(method=\"bfill\")\n",
    "df_adm[\"DAYS_NEXT_ADMIT\"] = (\n",
    "    df_adm.NEXT_ADMITTIME - df_adm.DISCHTIME\n",
    ").dt.total_seconds() / (24 * 60 * 60)\n",
    "df_adm[\"readmission_label\"] = (df_adm.DAYS_NEXT_ADMIT < 30).astype(\"int\")\n",
    "### filter out newborn and death\n",
    "df_adm = df_adm[df_adm[\"ADMISSION_TYPE\"] != \"NEWBORN\"]\n",
    "df_adm[\"DURATION\"] = (\n",
    "    df_adm[\"DISCHTIME\"] - df_adm[\"ADMITTIME\"]\n",
    ").dt.total_seconds() / (24 * 60 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding clinical codes to dataset\n",
    "# add diagnoses\n",
    "code = \"ICD9_CODE\"\n",
    "diagnoses = read_icd_diagnoses_table()\n",
    "diagnoses = filter_codes(diagnoses, code=code)\n",
    "diagnoses = group_by_return_col_list(diagnoses, [\"SUBJECT_ID\", \"HADM_ID\"], code)\n",
    "\n",
    "# add procedures\n",
    "procedures = read_icd_procedures_table()\n",
    "procedures = filter_codes(procedures, code=code)\n",
    "procedures = group_by_return_col_list(\n",
    "    procedures, [\"SUBJECT_ID\", \"HADM_ID\"], code, \"ICD9_CODE_PROCEDURE\"\n",
    ")\n",
    "\n",
    "# add cptevents\n",
    "code = \"CPT_CD\"\n",
    "cptevents = read_cptevents_table()\n",
    "cptevents = filter_codes(cptevents, code=code)\n",
    "cptevents = group_by_return_col_list(cptevents, [\"SUBJECT_ID\", \"HADM_ID\"], code)\n",
    "\n",
    "# add prescriptions\n",
    "code = \"NDC\"\n",
    "prescriptions = read_prescriptions_table()\n",
    "prescriptions = filter_codes(prescriptions, code=code)\n",
    "prescriptions = group_by_return_col_list(\n",
    "    prescriptions, [\"SUBJECT_ID\", \"HADM_ID\"], code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = read_patients_table()\n",
    "stays = read_icustays_table()\n",
    "stays = stays.merge(patients, how='inner', left_on=['SUBJECT_ID'], right_on=[\"SUBJECT_ID\"])\n",
    "cols = [\"SUBJECT_ID\", \"HADM_ID\"]\n",
    "stays = stays.merge(diagnoses, how=\"inner\", left_on=cols, right_on=cols)\n",
    "stays = stays.merge(procedures, how=\"inner\", left_on=cols, right_on=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = set(stays.SUBJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, c1, p1, m1 = (set(diagnoses.SUBJECT_ID), set(cptevents.SUBJECT_ID), \n",
    "                  set(procedures.SUBJECT_ID), set(prescriptions.SUBJECT_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d1 & s1), len(p1 & s1), len(p1 & s1 & d1), len((p1 & s1) & (d1 & d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"ICD9_CODE\"\n",
    "diagnoses = read_icd_diagnoses_table()\n",
    "diagnoses = filter_codes(diagnoses, code=code)\n",
    "diagnoses['ICD9_SHORT'] = diagnoses['ICD9_CODE'].apply(lambda x: x[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = group_by_return_col_list(diagnoses, [\"SUBJECT_ID\", \"HADM_ID\"], code)\n",
    "d2 = group_by_return_col_list(diagnoses, [\"SUBJECT_ID\", \"HADM_ID\"], 'ICD9_SHORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add procedures\n",
    "procedures = read_icd_procedures_table()\n",
    "procedures = filter_codes(procedures, code=code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedures['ICD9_PROC_SHORT'] = procedures['ICD9_CODE'].apply(lambda x: str(x)[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stays.ICD9_CODE.isna().sum(), stays.CPT_CD.isna().sum(), stays.NDC.isna().sum(), stays.ICD9_CODE_PROCEDURE.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stays = add_age_to_icustays(stays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df_adm.SUBJECT_ID) & set(procedures.SUBJECT_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df_adm.SUBJECT_ID) & set(stays.SUBJECT_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm = pd.merge(\n",
    "        df_adm, stays, on=[\"SUBJECT_ID\", \"HADM_ID\"], how=\"inner\"\n",
    "    )\n",
    "df_adm[\"ADMITTIME_C\"] = df_adm.ADMITTIME.apply(\n",
    "    lambda x: str(x).split(\" \")[0]\n",
    ")\n",
    "df_adm[\"ADMITTIME_C\"] = pd.to_datetime(\n",
    "    df_adm.ADMITTIME_C, format=\"%Y-%m-%d\", errors=\"coerce\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm.ICD9_CODE.isna().sum(), df_adm.ICD9_CODE_PROCEDURE.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def flatten(x):\n",
    "    return itertools.chain.from_iterable(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(flatten(df_adm[\"ICD9_CODE\"].dropna())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_cols = {\n",
    "        \"AGE\": [],\n",
    "        \"GENDER\": [],\n",
    "        \"LAST_CAREUNIT\": [],\n",
    "        \"MARITAL_STATUS\": [],\n",
    "        \"ETHNICITY\": [],\n",
    "        \"DISCHARGE_LOCATION\": [],\n",
    "    }\n",
    "df[\"GENDER\"], demographic_cols[\"GENDER\"] = pd.factorize(df[\"GENDER\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing ICD9_CODE codes occuring less than 5 times. \n",
      " num codes before filter: 6841 after filtering: 3511\n"
     ]
    }
   ],
   "source": [
    "code = \"ICD9_CODE\"\n",
    "diagnoses = read_icd_diagnoses_table()\n",
    "diagnoses = filter_codes(diagnoses, code=code)\n",
    "diagnoses['ICD9_SHORT'] = diagnoses['ICD9_CODE'].apply(lambda x: x[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=set(diagnoses.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=set(['ROW_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HADM_ID',\n",
       " 'ICD9_SHORT',\n",
       " 'ICD9_CODE',\n",
       " 'SEQ_NUM',\n",
       " 'LONG_TITLE',\n",
       " 'SHORT_TITLE',\n",
       " 'SUBJECT_ID']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedures = read_icd_procedures_table()\n",
    "procedures = filter_codes(procedures, code=code)\n",
    "procedures['ICD9_PROC_SHORT'] = procedures['ICD9_CODE'].apply(lambda x: str(x)[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_codes, diag_codes = list(set(procedures['ICD9_PROC_SHORT'])), list(set(diagnoses['ICD9_SHORT']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish a mapping from codes to integers\n",
    "map_proc = {}\n",
    "for i, key in enumerate(proc_codes):\n",
    "    map_proc[key] = i\n",
    "procedures['ICD9_PROC_SHORT'] = procedures['ICD9_PROC_SHORT'].apply(lambda x: map_proc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_diag = {}\n",
    "mapping_shift = len(proc_codes) # make sure the mapping will not mix\n",
    "for i, key in enumerate(diag_codes):\n",
    "    map_diag[key] = i + mapping_shift\n",
    "diagnoses['ICD9_SHORT'] = diagnoses['ICD9_SHORT'].apply(lambda x: map_diag[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87, 722)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_shift, len(diag_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    627575.000000\n",
       "mean        452.904784\n",
       "std         219.516260\n",
       "min          87.000000\n",
       "25%         247.000000\n",
       "50%         442.000000\n",
       "75%         685.000000\n",
       "max         808.000000\n",
       "Name: ICD9_SHORT, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnoses['ICD9_SHORT'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 1055, 87)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p1), len(set(procedures['ICD9_CODE'])), len(proc_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader.utils.vocab import Vocab\n",
    "cpt_vocab = Vocab()\n",
    "diag_vocab = Vocab()\n",
    "med_vocab = Vocab()\n",
    "proc_vocab = Vocab()\n",
    "diag_vocab._build_from_file(\"vocab/diag.vocab\")\n",
    "cpt_vocab._build_from_file(\"vocab/cpt.vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt = list(set(cptevents['CPT_CD']))\n",
    "diag = list(set(diagnoses[\"ICD9_CODE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctok = [cpt_vocab.convert_to_ids(str(c), \"C\", False) for c in cpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtok = [diag_vocab.convert_to_ids(d, \"D\", True) for d in diag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = list(set(df[\"SUBJECT_ID\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_df = df[df[\"SUBJECT_ID\"] == pids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cptevents = group_by_return_col_list(cptevents, [\"SUBJECT_ID\", \"HADM_ID\"], code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1, d1 = set(procedures['ICD9_PROC_SHORT']), set(diagnoses['ICD9_SHORT'])\n",
    "total = list(p1 | d1)\n",
    "dic_map = {}\n",
    "for i, key in enumerate(total):\n",
    "    dic_map[key] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dic_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedures = group_by_return_col_list(\n",
    "        procedures, [\"SUBJECT_ID\", \"HADM_ID\"], 'ICD9_PROC_SHORT'\n",
    "    )\n",
    "diagnoses = group_by_return_col_list(diagnoses, [\"SUBJECT_ID\", \"HADM_ID\"], 'ICD9_SHORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = read_patients_table()\n",
    "stays = read_icustays_table()\n",
    "stays = stays.merge(patients, how='inner', left_on=['SUBJECT_ID'], right_on=[\"SUBJECT_ID\"])\n",
    "cols = [\"SUBJECT_ID\", \"HADM_ID\"]\n",
    "stays = stays.merge(diagnoses, how=\"inner\", left_on=cols, right_on=cols)\n",
    "stays = stays.merge(procedures, how=\"inner\", left_on=cols, right_on=cols)\n",
    "stays = add_age_to_icustays(stays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm = pd.merge(\n",
    "        df_adm, stays, on=[\"SUBJECT_ID\", \"HADM_ID\"], how=\"inner\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = tmp['ICD9_PROC_SHORT'].values\n",
    "d1 = tmp['ICD9_SHORT'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2, u2 = pd.factorize(procedures['ICD9_PROC_SHORT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"SUBJECT_ID\", \"HADM_ID\"]\n",
    "tmp = procedures.merge(diagnoses, how='inner', left_on=cols, right_on=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = stays[['ICD9_PROC_SHORT', 'ICD9_SHORT']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('adm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, len(set(df['SUBJECT_ID'])), len(set(df['HADM_ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=procedures['ICD9_PROC_SHORT'].apply(lambda x: dic_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=diagnoses['ICD9_SHORT'].apply(lambda x: dic_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(p1) & set(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1[['ICD9_PROC_SHORT', 'ICD9_SHORT']].values.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses.ICD9_SHORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedures.ICD9_PROC_SHORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "class SeqCodeDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        batch_size,\n",
    "        train=True,\n",
    "        med=False,\n",
    "        diag=False,\n",
    "        proc=False,\n",
    "        split_num=2,\n",
    "    ):\n",
    "        self.proc = proc\n",
    "        self.med = med\n",
    "        self.diag = diag\n",
    "        \n",
    "        self.train = train\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.data = pickle.load(open(os.path.join(data_path, \"data_icd.pkl\"), \"rb\"))\n",
    "        self.data_info = self.data[\"info\"]\n",
    "        self.data = self.data[\"data\"]\n",
    "\n",
    "        data_split_path = os.path.join(\n",
    "            data_path, \"splits\", \"split_{}.pkl\".format(split_num)\n",
    "        )\n",
    "        if os.path.exists(data_split_path):\n",
    "            self.train_idx, self.valid_idx = pickle.load(open(data_split_path, \"rb\"))\n",
    "\n",
    "        self.keys = self._get_keys()\n",
    "\n",
    "        self.max_len = self._findmax_len()\n",
    "\n",
    "        self.num_dcodes = self.data_info['num_icd9_codes']\n",
    "        self.num_pcodes = self.data_info['num_proc_codes']\n",
    "    \n",
    "        self.num_codes = (\n",
    "            self.diag * self.num_dcodes\n",
    "            + self.proc * self.num_pcodes\n",
    "        )\n",
    "\n",
    "        self.demographics_shape = self.data_info[\"demographics_shape\"]\n",
    "\n",
    "    def _gen_idx(self, keys, min_adm=2):\n",
    "        idx = []\n",
    "        for k in keys:\n",
    "            v = self.data[k]\n",
    "            if len(v) < min_adm:\n",
    "                continue\n",
    "            for i, _ in enumerate(v):\n",
    "                idx.append((k, i))\n",
    "        return idx\n",
    "\n",
    "    def _get_keys(self, min_adm=2):\n",
    "        keys = []\n",
    "        for k, v in self.data.items():\n",
    "            if len(v) < min_adm:\n",
    "                continue\n",
    "            keys.append(k)\n",
    "        return keys\n",
    "\n",
    "    def _findmax_len(self):\n",
    "        m = 0\n",
    "        for v in self.data.values():\n",
    "            if len(v) > m:\n",
    "                m = len(v)\n",
    "        return m\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.keys)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        x = self.preprocess(self.data[k])\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def preprocess(self, seq):\n",
    "        \"\"\"create one hot vector of idx in seq, with length self.num_codes\n",
    "\n",
    "        Args:\n",
    "            seq: list of ideces where code should be 1\n",
    "\n",
    "        Returns:\n",
    "            x: one hot vector\n",
    "            ivec: vector for learning code representation\n",
    "            jvec: vector for learning code representation\n",
    "        \"\"\"\n",
    "\n",
    "        icd_one_hot = torch.zeros((self.num_codes, self.max_len), dtype=torch.long)\n",
    "        demo_one_hot = torch.zeros((self.demographics_shape, self.max_len), dtype=torch.long)\n",
    "        mask = torch.zeros((self.max_len,), dtype=torch.long)\n",
    "        ivec = []\n",
    "        jvec = []\n",
    "        for i, s in enumerate(seq):\n",
    "            icd = s['icd']\n",
    "            demo = s[\"demographics\"]\n",
    "            l = [\n",
    "                 s[\"diagnoses\"] * self.diag, \n",
    "                 s[\"procedures\"] * self.proc\n",
    "            ]\n",
    "            icd = list(itertools.chain.from_iterable(l))\n",
    "            \n",
    "            icd_one_hot[icd, i] = 1\n",
    "            demo_one_hot[:, i] = torch.Tensor(demo)\n",
    "            mask[i] = 1\n",
    "            for j in icd:\n",
    "                for k in icd:\n",
    "                    if j == k:\n",
    "                        continue\n",
    "                    ivec.append(j)\n",
    "                    jvec.append(k)\n",
    "            print(icd)\n",
    "                \n",
    "        return icd_one_hot.t(), mask, torch.LongTensor(ivec), torch.LongTensor(jvec), demo_one_hot.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/output\"\n",
    "batch_size = 32\n",
    "train = True\n",
    "\n",
    "d = SeqCodeDataset(data_path,\n",
    "                      batch_size,\n",
    "                      diag = True,\n",
    "                      proc = True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[680, 734, 325, 407, 39, 56, 29]\n",
      "[165, 674, 734, 782, 680, 352, 273, 764, 74, 56, 54]\n"
     ]
    }
   ],
   "source": [
    "x, m, i, j, dx = d.preprocess(d.data[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 54])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/output/splits/split_1.pkl\", \"rb\") as f:\n",
    "    splits = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08337452628110067"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits[1])/(len(splits[0])+len(splits[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/output/data_icd.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    return itertools.chain.from_iterable(x)\n",
    "len(set(flatten(df1[\"ICD9_PROC_SHORT\"].dropna()))), len(set(flatten(df1[\"ICD9_SHORT\"].dropna())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence dataloader for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "class SeqClassificationDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        batch_size,\n",
    "        y_label=\"los\",\n",
    "        train=True,\n",
    "        balanced_data=False,\n",
    "        validation_split=0.0,\n",
    "        split_num=1,\n",
    "        med=False,\n",
    "        diag=True,\n",
    "        proc=True,\n",
    "        cptcode=False\n",
    "    ):\n",
    "        super(SeqClassificationDataset).__init__()\n",
    "        self.proc = proc\n",
    "        self.med = med\n",
    "        self.diag = diag\n",
    "        self.cpt = cptcode\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.train = train\n",
    "        self.y_label = y_label\n",
    "        self.validation_split = validation_split\n",
    "        self.balanced_data = balanced_data\n",
    "        self.data = pickle.load(open(os.path.join(self.data_path, \"data_icd.pkl\"), \"rb\"))\n",
    "        self.data_info = self.data[\"info\"]\n",
    "        self.data = self.data[\"data\"]\n",
    "\n",
    "        self.demographics_shape = self.data_info[\"demographics_shape\"]\n",
    "\n",
    "        self.keys = list(map(int, self.data.keys()))\n",
    "        self.max_len = self._findmax_len()\n",
    "\n",
    "        self.num_dcodes = self.data_info['num_icd9_codes']\n",
    "        self.num_pcodes = self.data_info['num_proc_codes']\n",
    "        self.num_mcodes = self.data_info['num_med_codes']\n",
    "        self.num_ccodes = self.data_info['num_cpt_codes']\n",
    "        \n",
    "        self.num_codes = (\n",
    "            self.diag * self.num_dcodes\n",
    "            + self.cpt * self.num_ccodes\n",
    "            + self.proc * self.num_pcodes\n",
    "            + self.med * self.num_mcodes\n",
    "        )  \n",
    "\n",
    "        data_split_path = os.path.join(\n",
    "            self.data_path, \"splits\", \"split_{}.pkl\".format(split_num)\n",
    "        )\n",
    "        if os.path.exists(data_split_path):\n",
    "            self.train_idx, self.valid_idx = pickle.load(open(data_split_path, \"rb\"))\n",
    "            # select patients with at least two admissions\n",
    "            self.train_indices = self._gen_indices(self.train_idx)\n",
    "            self.valid_indices = self._gen_indices(self.valid_idx)\n",
    "            # re-label the patient ID!\n",
    "            # only patients with at least two visits are kept\n",
    "            self.train_idx = np.arange(len(self.train_indices))\n",
    "            self.valid_idx = len(self.train_indices) + np.arange(len(self.valid_indices))\n",
    "\n",
    "            if self.balanced_data:\n",
    "                self.train_idx = self._gen_balanced_indices(self.train_idx)\n",
    "                #self.valid_idx = self._gen_balanced_indices(self.valid_idx)\n",
    "        else:\n",
    "            # TODO: data index logic if train, validation splits are not provided\n",
    "            pass\n",
    "\n",
    "    def _gen_balanced_indices(self, indices):\n",
    "        \"\"\"Generate a balanced set of indices\"\"\"\n",
    "        ind_idx = {}\n",
    "\n",
    "        for idx in indices:\n",
    "            label = self.get_label(idx)\n",
    "            if label not in ind_idx:\n",
    "                ind_idx[label] = [idx]\n",
    "            else:\n",
    "                ind_idx[label].append(idx)\n",
    "\n",
    "        tr = []\n",
    "        te = []\n",
    "\n",
    "        lens = sorted([len(v) for v in ind_idx.values()])\n",
    "\n",
    "        if len(lens) > 3:\n",
    "            num_samples = lens[-2]\n",
    "        else:\n",
    "            num_samples = lens[0]\n",
    "\n",
    "        for v in ind_idx.values():\n",
    "            v = np.asarray(v)\n",
    "\n",
    "            if len(v) > num_samples:\n",
    "                v = v[np.random.choice(np.arange(len(v)), num_samples)]\n",
    "\n",
    "            # train, test = train_test_split(v, test_size=self.validation_split, random_state=1)\n",
    "            # te.append(test)\n",
    "\n",
    "            tr.append(v)\n",
    "\n",
    "        train = np.concatenate(tr)\n",
    "        # test = np.concatenate(te)\n",
    "        return train  # , test\n",
    "\n",
    "    def _gen_indices(self, keys):\n",
    "        indices = []\n",
    "        for k in keys:\n",
    "            v = self.data[k]\n",
    "            for j in range(len(v)):\n",
    "                if (j + 1) == len(v):\n",
    "                    continue\n",
    "                indices.append([k, j + 1])\n",
    "        return indices\n",
    "    \n",
    "    def _findmax_len(self):\n",
    "        \"\"\"Find the max number of visits of any patients\n",
    "\n",
    "        Returns:\n",
    "            [int]: the max number of visits\n",
    "        \"\"\"        \n",
    "        m = 0\n",
    "        for v in self.data.values():\n",
    "            if len(v) > m:\n",
    "                m = len(v)\n",
    "        return m\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index in self.train_idx:\n",
    "            idx = self.train_indices[index]\n",
    "        else:\n",
    "            idx = self.valid_indices[index - len(self.train_indices)]\n",
    "        x = self.preprocess(idx)\n",
    "        return x\n",
    "\n",
    "    def preprocess(self, idx):\n",
    "        \"\"\"n: total # of visits per each patients minus one\n",
    "            it's also the index for the last visits for extracting label y[n]\n",
    "        Args:\n",
    "            idx ([type]): [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"        \n",
    "        seq = self.data[idx[0]]\n",
    "        n = idx[1]\n",
    "        x_codes = torch.zeros((self.num_codes, self.max_len), dtype=torch.float)\n",
    "        demo = torch.Tensor(seq[n][\"demographics\"])\n",
    "        for i in range(n):\n",
    "            if (i + 1) == len(seq):\n",
    "                continue\n",
    "            s = seq[i]   \n",
    "            codes = [\n",
    "                 s[\"diagnoses\"] * self.diag, \n",
    "                 s[\"procedures\"] * self.proc\n",
    "            ]\n",
    "            codes = list(itertools.chain.from_iterable(codes))\n",
    "            x_codes[codes, i] = 1\n",
    "\n",
    "        x_cl = torch.Tensor(\n",
    "            [\n",
    "                n,\n",
    "            ]\n",
    "        )\n",
    "       \n",
    "        if self.y_label == \"los\":\n",
    "            los = seq[n][\"los\"]\n",
    "            if los != los:\n",
    "                los = 9\n",
    "            y = torch.Tensor([los - 1])\n",
    "        elif self.y_label == \"readmission\":\n",
    "            y = torch.Tensor([seq[n][\"readmission\"]])\n",
    "        else:\n",
    "            y = torch.Tensor([seq[n][\"mortality\"]])\n",
    "\n",
    "        return (x_codes.t(), x_cl, demo, y)\n",
    "\n",
    "    def get_label(self, idx):\n",
    "        if idx in self.train_idx:\n",
    "            idx = self.train_indices[idx]\n",
    "        else:\n",
    "            idx = self.valid_indices[idx - len(self.train_indices)]\n",
    "        seq = self.data[idx[0]]\n",
    "        n = idx[1]\n",
    "        if self.y_label == \"los\":\n",
    "            los = seq[n][\"los\"]\n",
    "            if los != los:\n",
    "                los = 9\n",
    "            y = torch.Tensor([los - 1])\n",
    "        elif self.y_label == \"readmission\":\n",
    "            y = torch.Tensor([seq[n][\"readmission\"]])\n",
    "        else:\n",
    "            y = torch.Tensor([seq[n][\"mortality\"]])\n",
    "        y = y.item()\n",
    "        return y\n",
    "\n",
    "    def __len__(self):\n",
    "        l = 0\n",
    "        if self.train:\n",
    "            l = len(self.train_idx)\n",
    "        else:\n",
    "            l = len(self.valid_idx)\n",
    "\n",
    "        return l\n",
    "\n",
    "def collate_fn(data):\n",
    "    x_codes, x_cl,  demo, y_code = zip(*data)\n",
    "    x_codes = torch.stack(x_codes, dim=1)\n",
    "    demo = torch.stack(demo, dim=0)\n",
    "    y_code = torch.stack(y_code, dim=1).long()\n",
    "    x_cl = torch.stack(x_cl, dim=0).long()\n",
    "    b_is = torch.arange(x_cl.shape[0]).reshape(tuple(x_cl.shape)).long()\n",
    "    return (\n",
    "        x_codes,\n",
    "        x_cl.squeeze(),\n",
    "        b_is.squeeze(),\n",
    "        demo,\n",
    "    ), y_code.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/output\"\n",
    "data_split_path\n",
    "batch_size = 32\n",
    "train = True\n",
    "\n",
    "d = SeqClassificationDataset(data_path,\n",
    "                      batch_size,\n",
    "                      y_label=\"mortality\",\n",
    "                      diag = True,\n",
    "                      proc = True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_num = 0\n",
    "data_split_path = os.path.join(\n",
    "            data_path, \"splits\", \"split_{}.pkl\".format(split_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/output/splits/split_0.pkl'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_split_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, valid_idx = pickle.load(open(data_split_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([54994, 93381, 23990, ...,  7059,   376, 82202])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 7630, 7631, 7632])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mortality'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([d.data[x][0][d.y_label] for x in train_idx]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6069,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_codes, x_cl,  demo, y_code = d[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 809])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence classification trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from base import BaseTrainer\n",
    "from model.metric import roc_auc_1, pr_auc_1, pr_auc, roc_auc\n",
    "\n",
    "\n",
    "class ClassificationTrainer(BaseTrainer):\n",
    "    \"\"\"\n",
    "    Trainer class\n",
    "\n",
    "    Note:\n",
    "        Inherited from BaseTrainer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        loss,\n",
    "        metrics,\n",
    "        optimizer,\n",
    "        resume,\n",
    "        config,\n",
    "        data_loader,\n",
    "        valid_data_loader=None,\n",
    "        lr_scheduler=None,\n",
    "        train_logger=None,\n",
    "    ):\n",
    "        super(ClassificationTrainer, self).__init__(\n",
    "            model, loss, metrics, optimizer, resume, config, train_logger\n",
    "        )\n",
    "        self.config = config\n",
    "        self.data_loader = data_loader\n",
    "        self.valid_data_loader = valid_data_loader\n",
    "        self.do_validation = self.valid_data_loader is not None\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.log_step = config[\"trainer\"].get(\n",
    "            \"log_step\", int(np.sqrt(data_loader.batch_size))\n",
    "        )\n",
    "\n",
    "        if self.config[\"model\"][\"args\"][\"num_classes\"] == 1:\n",
    "            weight_0 = self.config[\"trainer\"].get(\"class_weight_0\", 1.0)\n",
    "            weight_1 = self.config[\"trainer\"].get(\"class_weight_1\", 1.0)\n",
    "            self.weight = [weight_0, weight_1]\n",
    "            self.loss = lambda output, target: loss(output, target, self.weight)\n",
    "        self.prauc_flag = pr_auc in self.metrics and roc_auc in self.metrics\n",
    "\n",
    "    def _eval_metrics(self, output, target, **kwargs):\n",
    "        acc_metrics = np.zeros(len(self.metrics))\n",
    "        for i, metric in enumerate(self.metrics):\n",
    "            acc_metrics[i] += metric(output, target, **kwargs)\n",
    "            self.writer.add_scalar(f\"{metric.__name__}\", acc_metrics[i])\n",
    "        return acc_metrics\n",
    "\n",
    "    def _train_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        Training logic for an epoch\n",
    "\n",
    "        :param epoch: Current training epoch.\n",
    "        :return: A log that contains all information you want to save.\n",
    "\n",
    "        Note:\n",
    "            If you have additional information to record, for example:\n",
    "                > additional_log = {\"x\": x, \"y\": y}\n",
    "            merge it with log before return. i.e.\n",
    "                > log = {**log, **additional_log}\n",
    "                > return log\n",
    "\n",
    "            The metrics in log must have the key 'metrics'.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_metrics = np.zeros(len(self.metrics))\n",
    "        all_t = []\n",
    "        all_o = []\n",
    "        for batch_idx, (data, target) in enumerate(self.data_loader):\n",
    "\n",
    "            all_t.append(target.numpy())\n",
    "\n",
    "            target = target.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output, _ = self.model(data, device=self.device)\n",
    "            loss = self.loss(output, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.writer.set_step((epoch - 1) * len(self.data_loader) + batch_idx)\n",
    "            self.writer.add_scalar(\"loss\", loss.item())\n",
    "\n",
    "            total_loss += loss\n",
    "            total_metrics += self._eval_metrics(output, target)\n",
    "            all_o.append(output.detach().cpu().numpy())\n",
    "\n",
    "            if self.verbosity >= 2 and batch_idx % self.log_step == 0:\n",
    "                self.logger.info(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)] {}: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * self.data_loader.batch_size,\n",
    "                        self.data_loader.n_samples,\n",
    "                        100.0 * batch_idx / len(self.data_loader),\n",
    "                        \"loss\",\n",
    "                        loss,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        total_metrics = total_metrics / len(self.data_loader)\n",
    "        if self.prauc_flag:\n",
    "            all_o = np.hstack(all_o)\n",
    "            all_t = np.hstack(all_t)\n",
    "            total_metrics[-2] = pr_auc_1(all_o, all_t)\n",
    "            total_metrics[-1] = roc_auc_1(all_o, all_t)\n",
    "\n",
    "        log = {\n",
    "            \"loss\": total_loss / len(self.data_loader),\n",
    "            \"metrics\": total_metrics,\n",
    "        }\n",
    "\n",
    "        if self.do_validation:\n",
    "            val_log = self._valid_epoch(epoch)\n",
    "            log = {**log, **val_log}\n",
    "\n",
    "        if self.lr_scheduler is not None:\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "        return log\n",
    "\n",
    "    def _valid_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        Validate after training an epoch\n",
    "\n",
    "        :return: A log that contains information about validation\n",
    "\n",
    "        Note:\n",
    "            The validation metrics in log must have the key 'val_metrics'.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_val_metrics = np.zeros(len(self.metrics))\n",
    "        all_t = []\n",
    "        all_o = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(self.valid_data_loader):\n",
    "                all_t.append(target.numpy())\n",
    "                target = target.to(self.device)\n",
    "\n",
    "                output, _ = self.model(data, self.device)\n",
    "                loss = self.loss(\n",
    "                    output,\n",
    "                    target.reshape(\n",
    "                        -1,\n",
    "                    ),\n",
    "                )\n",
    "                all_o.append(output.detach().cpu().numpy())\n",
    "\n",
    "                self.writer.set_step(\n",
    "                    (epoch - 1) * len(self.valid_data_loader) + batch_idx, \"valid\"\n",
    "                )\n",
    "                self.writer.add_scalar(\"loss\", loss.item())\n",
    "                total_val_loss += loss.item()\n",
    "                total_val_metrics += self._eval_metrics(output, target)\n",
    "\n",
    "        total_val_metrics = (total_val_metrics / len(self.valid_data_loader)).tolist()\n",
    "        if self.prauc_flag:\n",
    "            all_o = np.hstack(all_o)\n",
    "            all_t = np.hstack(all_t)\n",
    "            total_val_metrics[-2] = pr_auc_1(all_o, all_t)\n",
    "            total_val_metrics[-1] = roc_auc_1(all_o, all_t)\n",
    "\n",
    "        return {\n",
    "            \"val_loss\": total_val_loss / len(self.valid_data_loader),\n",
    "            \"val_metrics\": total_val_metrics,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('configs/taper/seq_mortality.json'))\n",
    "data_loader = \n",
    "cl = ClassificationTrainer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
